{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0wPO3wcAOy/ar2if4LUrx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEfcwPBQ8w7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tea is healthy and calming, don't you think?\")"
      ],
      "metadata": {
        "id": "JxGv0rL-RIqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "id": "L-p0ZHs1RKN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't is considered 2 tokens (do and n't)."
      ],
      "metadata": {
        "id": "rjYha9teRohW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "8QAHwcppRcV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
        "print(\"-\"*40)\n",
        "for token in doc:\n",
        "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
      ],
      "metadata": {
        "id": "35ArJ8tiS-xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
      ],
      "metadata": {
        "id": "LNyZzBKpT_l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
        "patterns = [nlp(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", patterns)"
      ],
      "metadata": {
        "id": "wbWWTvn7UiTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
        "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
        "               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\") \n",
        "matches = matcher(text_doc)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q58MXiicU84b",
        "outputId": "196eda33-8579-4f4c-e8b3-4bdeaa8849ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_id, start, end = matches[0]\n",
        "print(nlp.vocab.strings[match_id], text_doc[start:end])"
      ],
      "metadata": {
        "id": "oylVtrb0VUBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# item_ratings is a dictionary of lists. If a key doesn't exist in item_ratings,\n",
        "# the key is added with an empty list as the value.\n",
        "item_ratings = defaultdict(list)\n",
        "\n",
        "for idx, review in data.iterrows():\n",
        "    doc = nlp(review.text)\n",
        "    # Using the matcher from the previous exercise\n",
        "    matches = matcher(doc)\n",
        "    \n",
        "    # Create a set of the items found in the review text\n",
        "    found_items = set([doc[match[1]:match[2]].text.lower() for match in matches])\n",
        "    \n",
        "    # Update item_ratings with rating for each item in found_items\n",
        "    # Transform the item strings to lowercase to make it case insensitive\n",
        "    for item in found_items:\n",
        "        item_ratings[item].append(review.stars)\n"
      ],
      "metadata": {
        "id": "LMniohhvzExf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean ratings for each menu item as a dictionary\n",
        "mean_ratings = {item: sum(ratings)/len(ratings) for item, ratings in item_ratings.items() }\n",
        "\n",
        "# Find the worst item, and write it as a string in worst_item. This can be multiple lines of code if you want.\n",
        "worst_item = sorted(mean_ratings, key= mean_ratings.get)[0]\n"
      ],
      "metadata": {
        "id": "ltkQQb-fzD4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Detector"
      ],
      "metadata": {
        "id": "UqC3nrx2UL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading the spam data\n",
        "# ham is the label for non-spam messages\n",
        "spam = pd.read_csv('../input/nlp-course/spam.csv')"
      ],
      "metadata": {
        "id": "3ck-276PU-Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Create an empty model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add the TextCategorizer to the empty model\n",
        "textcat = nlp.add_pipe(\"textcat\")"
      ],
      "metadata": {
        "id": "mjdSYy_kUAlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels to text classifier\n",
        "textcat.add_label(\"ham\")\n",
        "textcat.add_label(\"spam\")"
      ],
      "metadata": {
        "id": "ks4U0P3gULKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = spam['text'].values\n",
        "train_labels = [{'cats': {'ham': label == 'ham',\n",
        "                          'spam': label == 'spam'}} \n",
        "                for label in spam['label']]"
      ],
      "metadata": {
        "id": "2u2dlBJMU2lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we combine the texts and labels into a single list."
      ],
      "metadata": {
        "id": "atXT774iV3zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = list(zip(train_texts, train_labels))\n",
        "train_data[:3]"
      ],
      "metadata": {
        "id": "hxukBaBBV0x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.util import minibatch\n",
        "from spacy.training.example import Example\n",
        "\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# Create the batch generator with batch size = 8\n",
        "batches = minibatch(train_data, size=8)\n",
        "# Iterate through minibatches\n",
        "for batch in batches:\n",
        "    # Each batch is a list of (text, label) \n",
        "    for text, labels in batch:\n",
        "        doc = nlp.make_doc(text)\n",
        "        example = Example.from_dict(doc, labels)\n",
        "        nlp.update([example], sgd=optimizer)"
      ],
      "metadata": {
        "id": "vHJHfnXHYhhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just one training loop (or epoch) through the data. The model will typically need multiple epochs. Use another loop for more epochs, and optionally re-shuffle the training data at the begining of each loop."
      ],
      "metadata": {
        "id": "infpMvrwe4mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "losses = {}\n",
        "for epoch in range(10):\n",
        "    random.shuffle(train_data)\n",
        "    # Create the batch generator with batch size = 8\n",
        "    batches = minibatch(train_data, size=8)\n",
        "    # Iterate through minibatches\n",
        "    for batch in batches:\n",
        "        for text, labels in batch:\n",
        "            doc = nlp.make_doc(text)\n",
        "            example = Example.from_dict(doc, labels)\n",
        "            nlp.update([example], sgd=optimizer, losses=losses)\n",
        "    print(losses)"
      ],
      "metadata": {
        "id": "Rzef-HUme6Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
        "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
        "docs = [nlp.tokenizer(text) for text in texts]\n",
        "    \n",
        "# Use textcat to get the scores for each doc\n",
        "textcat = nlp.get_pipe('textcat')\n",
        "scores = textcat.predict(docs)\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "LsDU_71cfkYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From the scores, find the label with the highest score/probability\n",
        "predicted_labels = scores.argmax(axis=1)\n",
        "print([textcat.labels[label] for label in predicted_labels])"
      ],
      "metadata": {
        "id": "d45QY_Ykglfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}