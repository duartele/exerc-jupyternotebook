{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Cleaning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/DTb42uVhiqLTmMuReXFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duartele/exerc-jupyternotebook/blob/main/Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj_8zxCCN0HM"
      },
      "source": [
        "## Handling missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9hKS9d64Akb"
      },
      "source": [
        "# get the number of missing data points per column\n",
        "missing_values_count = nfl_data.isnull().sum()\n",
        "\n",
        "# how many total missing values do we have?\n",
        "total_cells = np.product(nfl_data.shape)\n",
        "total_missing = missing_values_count.sum()\n",
        "\n",
        "# percent of data that is missing\n",
        "percent_missing = (total_missing/total_cells) * 100\n",
        "print(percent_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKYkCP8or7jc"
      },
      "source": [
        "# replace all NA's the value that comes directly after it in the same column, \n",
        "# then replace all the remaining na's with 0\n",
        "subset_nfl_data.fillna(method='bfill', axis=0).fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4PQPH_MNhCv"
      },
      "source": [
        "## Scaling and normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dDUAmpcRjMN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4qLg_jODd0"
      },
      "source": [
        "# for Box-Cox Transformation\n",
        "from scipy import stats\n",
        "# for min_max scaling\n",
        "from mlxtend.preprocessing import minmax_scaling\n",
        "\n",
        "# set seed for reproducibility\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O5a5sKARvz0"
      },
      "source": [
        " In scaling, you're changing the range of your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC_dvJujQzVQ"
      },
      "source": [
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.exponential(size=1000)\n",
        "\n",
        "# mix-max scale the data between 0 and 1\n",
        "scaled_data = minmax_scaling(original_data, columns=[0])\n",
        "\n",
        "# plot both together to compare\n",
        "fig, ax = plt.subplots(1,2)\n",
        "sns.distplot(original_data, ax=ax[0])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "sns.distplot(scaled_data, ax=ax[1])\n",
        "ax[1].set_title(\"Scaled data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYoFHpZURpXq"
      },
      "source": [
        "In normalization, you're changing the shape of the distribution of your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSjOCz-QR-b0"
      },
      "source": [
        "# normalize the exponential data with boxcox\n",
        "normalized_data = stats.boxcox(original_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caXif0frvdPY"
      },
      "source": [
        "# Parsing Dates - it will be \"object\" if you don't parse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FYFbJ_vcq6"
      },
      "source": [
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnQoGeQ3xoB1"
      },
      "source": [
        "if a date is MM/DD/YY (02/25/17) the format is \"%m/%d/%y\"\n",
        "\"%Y\" (upper y) is used if the year has four digits (2017)\n",
        "That is (02/25/2017) the format is \"%m/%d/%Y\"\n",
        "Other formats: DD/MM/YY (25/02/17 - \"%d/%m/%y\") ; DD-MM-YY (25-02-17 - \"%d-%m-%y\"). At the end, the date will be show as the defaut YYYY-MM-DD (datetime64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0j9jdgUvjwe"
      },
      "source": [
        "# create a new column, date_parsed, with the parsed dates\n",
        "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3aInXqdzjgk"
      },
      "source": [
        "If your dates is with multiple formats, you can use \"infer_datetime_format\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diV-PGrGzXPp"
      },
      "source": [
        "landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZWROS6i0T0b"
      },
      "source": [
        "day_of_month_landslides = landslides['date_parsed'].dt.day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paHywKfa3Ga7"
      },
      "source": [
        "To check if everything looks right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxzN8cZq3MfU"
      },
      "source": [
        "date_lengths = earthquakes.Date.str.len()\n",
        "date_lengths.value_counts()\n",
        "\n",
        "#or showing trhough a histogram of the day (values must be in [0,31] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soduq68r3lfo"
      },
      "source": [
        "#In that example, 3 dates have len of 24. So we run this code\n",
        "indices = np.where([date_lengths == 24])[1]\n",
        "print('Indices with corrupted data:', indices)\n",
        "earthquakes.loc[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzSHM1sv4erP"
      },
      "source": [
        "#Fixing manually the incorrect dates\n",
        "earthquakes.loc[3378, \"Date\"] = \"02/23/1975\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECHjbP6KFyJq"
      },
      "source": [
        "# Character Encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwH8dZNVFxvC"
      },
      "source": [
        "# helpful character encoding module\n",
        "import chardet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNdh6s_5Jrk5"
      },
      "source": [
        "# look at the first ten thousand bytes to guess the character encoding\n",
        "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(10000))\n",
        "\n",
        "# check what the character encoding might be\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMHSVYlaJrVB"
      },
      "source": [
        "# read in the file with the encoding detected by chardet\n",
        "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
        "\n",
        "# look at the first few lines\n",
        "kickstarter_2016.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79-JfzYKPbJ"
      },
      "source": [
        "Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o2k4aspKQ31"
      },
      "source": [
        "#1 - class bytes We need to create a new_entry in bytes (UTF-8is defaut)\n",
        "sample_entry = b'\\xa7A\\xa6n'\n",
        "print(sample_entry)\n",
        "print('data type:', type(sample_entry))\n",
        "#solution - Try using .decode() to get the string, then .encode() to get the bytes representation, encoded in UTF-8.\n",
        "before = sample_entry.decode(\"big5-tw\")\n",
        "new_entry = before.encode()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zeuxamRN9TU"
      },
      "source": [
        "# Inconsistent Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3eu3lTKN8R3"
      },
      "source": [
        "# helpful modules\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import process\n",
        "import chardet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W_l4jbwLUpc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}