{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQL_course.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMHLu28nfmpnuMarQQg7uS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duartele/exerc-jupyternotebook/blob/main/SQL_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzrb97IY9Vo"
      },
      "source": [
        "# To use this commands, we need to use Kaggle's notebook because a client is requiered.\n",
        "\n",
        "Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset.\n",
        "\n",
        "\n",
        "## So I'm using this just to have a backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5PqbSQuXtot"
      },
      "source": [
        "from google.cloud import bigquery"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQsXSFqYA93"
      },
      "source": [
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAC_Plm-ZxoW"
      },
      "source": [
        "# Construct a reference to the \"hacker_news\" dataset\n",
        "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzLwqWV6bX2t"
      },
      "source": [
        "# List all the tables in the \"hacker_news\" dataset\n",
        "tables = list(client.list_tables(dataset))\n",
        "\n",
        "# Print names of all tables in the dataset (there are four!)\n",
        "for table in tables:  \n",
        "    print(table.table_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3lnE2GsbvGa"
      },
      "source": [
        "# Construct a reference to the \"full\" table\n",
        "table_ref = dataset_ref.table(\"full\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G4Ed8uZgqxc"
      },
      "source": [
        "# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
        "table.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zspPeocwhGib"
      },
      "source": [
        "Each SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:\n",
        "\n",
        "The name of the column\n",
        "The field type (or datatype) in the column\n",
        "The mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\n",
        "A description of the data in that column\n",
        "The first field has the SchemaField:\n",
        "\n",
        "SchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN_eQGExiUow"
      },
      "source": [
        "# Preview the first five lines of the \"full\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QpuXefi4OP"
      },
      "source": [
        "# Preview the first five entries in the \"title\" column of the \"full\" table\n",
        "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5qMuyQQDHOq"
      },
      "source": [
        "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
        "query = \"\"\"\n",
        "        SELECT city, country #more columns just use commas\n",
        "        #SELECT DISTINCT city #if you don't want repetitions\n",
        "        #SELECT * #for selecting all the columns\n",
        "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
        "        WHERE country = 'US'\n",
        "        \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuDTvTg7DbAw"
      },
      "source": [
        "To run this query we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmZmqFClDgzQ"
      },
      "source": [
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "# Set up the query\n",
        "query_job = client.query(query)\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "us_cities = query_job.to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZkIwrUDFZgb"
      },
      "source": [
        "# To check the size of a query before run it (for avoid exceeding the 5TB limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ3YxwasGT4Z"
      },
      "source": [
        "Estimate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EAakRFOFqaY"
      },
      "source": [
        "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
        "\n",
        "# API request - dry run query to estimate costs\n",
        "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
        "\n",
        "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ukyVArkGWf0"
      },
      "source": [
        "Put a Limit (1MB in this case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7M8zN_cGcyz"
      },
      "source": [
        "# Only run the query if it's less than 1 MB\n",
        "ONE_MB = 1000*1000\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
        "\n",
        "# Set up the query (will only run if it's less than 1 MB)\n",
        "safe_query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - try to run the query, and return a pandas DataFrame\n",
        "safe_query_job.to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ltCh-o45HH"
      },
      "source": [
        "## Agregate functions\n",
        "COUNT() SUM() AVG() MIN() MAX()\n",
        "\n",
        "HAVING is used in combination with GROUP BY to ignore groups that don't meet certain criteria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KTHXVsQ44kq"
      },
      "source": [
        "#SELECT Animal, COUNT(ID)\n",
        "#COUNT(1) YOU CAN USE COUNT(1) - MORE FAST AND EASY TO READ - COUNT EACH GROUP\n",
        "#FROM `table`\n",
        "#WHERE deleted = True\n",
        "#GROUP BY Animal\n",
        "#HAVING COUNT(ID) > 1\n",
        "#HAVING COUNT(1) > 1\n",
        "#ORDER BY Animal\n",
        "#ORDER BY Animal DESC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCOcPr_xxv0L"
      },
      "source": [
        "# DATE = YYYY-MM-DD\n",
        "\n",
        "We can use the function EXTRACT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdPEMGpxuZT"
      },
      "source": [
        "#SELECT Animal, EXTRACT(DAY from Date) as Day\n",
        "#SELECT Animal, EXTRACT(WEEK from Date) as Week- 1 is Sunday and 7 is Saturday"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gp-6wpO57S7"
      },
      "source": [
        "# 'AS' is used to Aliasing variables\n",
        "# Now we are taking a look in the  WITH...AS command\n",
        "\n",
        "WITH...AS create a temporary table (called CTE - Common Table Expression)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gGrHD0u560Y"
      },
      "source": [
        "query_with_CTE = \"\"\" \n",
        "                 WITH time AS \n",
        "                 (\n",
        "                     SELECT DATE(block_timestamp) AS trans_date\n",
        "                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n",
        "                 )\n",
        "                 SELECT COUNT(1) AS transactions,\n",
        "                        trans_date\n",
        "                 FROM time\n",
        "                 GROUP BY trans_date\n",
        "                 ORDER BY trans_date\n",
        "                 \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1twNLgUeBr"
      },
      "source": [
        "#Join\n",
        "\n",
        "INNER JOIN is used when a row will only be put in the final output table if the value in the columns you're using to combine them shows up in both the tables you're joining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIzGW_63Udky"
      },
      "source": [
        "query = \"\"\"\n",
        "        SELECT L.license, COUNT(1) AS number_of_files\n",
        "        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n",
        "        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n",
        "            ON sf.repo_name = L.repo_name\n",
        "        GROUP BY L.license\n",
        "        ORDER BY number_of_files DESC\n",
        "        \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}