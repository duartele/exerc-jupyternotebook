{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOU2UOcPa6AARU+LrE8gsM2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duartele/exerc-jupyternotebook/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L-pRxu_dCZ9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a network with 1 linear unit\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1, input_shape=[3])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the first argument, units, we define how many outputs we want. In this case we are just predicting 'calories', so we'll use units=1.\n",
        "\n",
        "With the second argument, input_shape, we tell Keras the dimensions of the inputs. Setting input_shape=[3] ensures the model will accept three features as input ('sugars', 'fiber', and 'protein')."
      ],
      "metadata": {
        "id": "Pvjo6xWJd2u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    # the linear output layer \n",
        "    layers.Dense(units=1),\n",
        "])"
      ],
      "metadata": {
        "id": "XSpNEa7zd8FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "layers.Dense(units=8),\n",
        "layers.Activation('relu')\n",
        "\n",
        "Is equivalent to\n",
        "layers.Dense(units=3, activation='relu')\n"
      ],
      "metadata": {
        "id": "jtl8c4ixgDiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Others activations 'relu' to 'elu', 'selu', 'swish'\n",
        "activation_layer = layers.Activation('relu')\n",
        "\n",
        "x = tf.linspace(-3.0, 3.0, 100)\n",
        "y = activation_layer(x) # once created, a layer is callable just like a function\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "plt.plot(x, y)\n",
        "plt.xlim(-3, 3)\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Output\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aof1rA18g4FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example."
      ],
      "metadata": {
        "id": "1IzSOKvgPQTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mae\",\n",
        ")"
      ],
      "metadata": {
        "id": "2OdB6O8TPPxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "ti5u8znLPw4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We'll convert the data to a Pandas dataframe, which makes the plotting easy."
      ],
      "metadata": {
        "id": "_uiVg5qkRjwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# convert the training history to a dataframe\n",
        "history_df = pd.DataFrame(history.history)\n",
        "# use Pandas native plot method\n",
        "history_df['loss'].plot();"
      ],
      "metadata": {
        "id": "BuOxFksmRk5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "smaller batch sizes gave noisier weight updates and loss curves. This is because each batch is a small sample of data and smaller samples tend to give noisier estimates. Smaller batches can have an \"averaging\" effect though which can be beneficial.\n",
        "Smaller learning rates make the updates smaller and the training takes longer to converge. Large learning rates can speed up training, but don't \"settle in\" to a minimum as well."
      ],
      "metadata": {
        "id": "2HC_5XwlWAHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset."
      ],
      "metadata": {
        "id": "epIUXuL-Xv9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "q2JIwvNBRlXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the callback, add it as an argument in fit (you can have several, so put it in a list). Choose a large number of epochs when using early stopping, more than you'll need."
      ],
      "metadata": {
        "id": "q5Jf3fOYAFIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=500,\n",
        "    callbacks=[early_stopping], # put your callbacks in a list\n",
        "    verbose=0,  # turn off training log = suppress output\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
      ],
      "metadata": {
        "id": "62AS92ScRldU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout - The same idea to random forests - minimizing an overfitting\n",
        "\n",
        "When adding dropout, you may need to increase the number of units in your Dense layers."
      ],
      "metadata": {
        "id": "kF8nmJ--Mhd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.Sequential([\n",
        "    # ...\n",
        "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])"
      ],
      "metadata": {
        "id": "NYc54ZQnRliQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization (or \"batchnorm\")"
      ],
      "metadata": {
        "id": "-3IrjuGgNVy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can put it after a layer...\n",
        "\n",
        "layers.Dense(16, activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        "\n",
        "#... or between a layer and its activation function:\n",
        "layers.Dense(16),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation('relu'),"
      ],
      "metadata": {
        "id": "y9nTZx5pRlnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Classification Problems, we use Cross-Entropy as Loss Function and Sigmoid as final activation function.\n"
      ],
      "metadata": {
        "id": "YPlqFga8SGST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
        "    layers.Dense(4, activation='relu'),    \n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "metadata": {
        "id": "bqxQLCT6TLJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "6IfnAExYRlsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "# Start the plot at epoch 5\n",
        "history_df.loc[5:, ['loss', 'val_loss']].plot()\n",
        "history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
        "\n",
        "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
        "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
        "      .format(history_df['val_loss'].min(), \n",
        "              history_df['val_binary_accuracy'].max()))"
      ],
      "metadata": {
        "id": "-LKysTSlUjeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}