{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNg0hCn6eRqWkhXItZxoes+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the data\n",
        "X_full = pd.read_csv('../input/train.csv', index_col='Id')\n",
        "X_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n",
        "\n",
        "# Obtain target and predictors\n",
        "y = X_full.SalePrice\n",
        "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
        "X = X_full[features].copy()\n",
        "X_test = X_test_full[features].copy()\n",
        "\n",
        "# Break off validation set from training data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
        "                                                      random_state=0)"
      ],
      "metadata": {
        "id": "HYU0Xr28_IjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5pKWDuR5XW1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define the models\n",
        "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
        "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
        "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
        "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
        "\n",
        "models = [model_1, model_2, model_3, model_4, model_5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function for comparing different models\n",
        "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
        "    model.fit(X_t, y_t)\n",
        "    preds = model.predict(X_v)\n",
        "    return mean_absolute_error(y_v, preds)\n",
        "\n",
        "for i in range(0, len(models)):\n",
        "    mae = score_model(models[i])\n",
        "    print(\"Model %d MAE: %d\" % (i+1, mae))"
      ],
      "metadata": {
        "id": "PQLsmjIs5mA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)"
      ],
      "metadata": {
        "id": "ctHgJFMa_d3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "my_model.fit(X, y)\n",
        "​\n",
        "# Generate test predictions\n",
        "preds_test = my_model.predict(X_test)\n",
        "​\n",
        "# Save predictions in format used for competition scoring\n",
        "output = pd.DataFrame({'Id': X_test.index,\n",
        "                       'SalePrice': preds_test})\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "91cUZaGS_yfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A way to fill NA"
      ],
      "metadata": {
        "id": "JUWghazTYafN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "my_imputer = SimpleImputer()\n",
        "#final_imputer = SimpleImputer(strategy='median')\n",
        "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
        "imputed_X_valid = pd.DataFrame(my_imputer.fit_transform(X_val))\n"
      ],
      "metadata": {
        "id": "outgLwtsYaCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Cardinality\" means the number of unique values in a column\n",
        "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
        "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
        "                        X_train_full[cname].dtype == \"object\"]"
      ],
      "metadata": {
        "id": "MqCXmleUyJKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The code cell below prints the unique entries\n",
        "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())"
      ],
      "metadata": {
        "id": "43wAUC1HF5G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and\n",
        "setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix)."
      ],
      "metadata": {
        "id": "qK07nFIu3vm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Apply one-hot encoder to each column with categorical data\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
        "\n",
        "# One-hot encoding removed index; put it back\n",
        "OH_cols_train.index = X_train.index\n",
        "OH_cols_valid.index = X_valid.index\n",
        "\n",
        "# Remove categorical columns (will replace with one-hot encoding)\n",
        "num_X_train = X_train.drop(object_cols, axis=1)\n",
        "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "# Add one-hot encoded columns to numerical features\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
      ],
      "metadata": {
        "id": "Me6yKYrn3y0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Make copy to avoid changing original data \n",
        "label_X_train = X_train.copy()\n",
        "label_X_valid = X_valid.copy()\n",
        "\n",
        "# Apply ordinal encoder to each column with categorical data\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
        "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])"
      ],
      "metadata": {
        "id": "eVsxFNtZ0bTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables."
      ],
      "metadata": {
        "id": "ykhHr7fZ1voo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of categorical variables\n",
        "s = (X_train.dtypes == 'object')\n",
        "object_cols = list(s[s].index)\n",
        "\n",
        "print(\"Categorical variables:\")\n",
        "print(object_cols)\n",
        "\n",
        "drop_X_train = X_train.select_dtypes(exclude=['object'])"
      ],
      "metadata": {
        "id": "Iu7rs1xwzXzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]"
      ],
      "metadata": {
        "id": "XIWCFZyWyRuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep selected columns only\n",
        "my_cols = low_cardinality_cols + numerical_cols\n",
        "X_train = X_train_full[my_cols].copy()\n",
        "X_valid = X_valid_full[my_cols].copy()"
      ],
      "metadata": {
        "id": "slUg1EszyZl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical columns in the training data\n",
        "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
        "\n",
        "# Columns that can be safely ordinal encoded\n",
        "good_label_cols = [col for col in object_cols if \n",
        "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
        "        \n",
        "# Problematic columns that will be dropped from the dataset\n",
        "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
        "        \n",
        "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
        "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
      ],
      "metadata": {
        "id": "QZdjuhzTHpUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of unique entries in each column with categorical data\n",
        "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
        "d = dict(zip(object_cols, object_nunique))\n",
        "\n",
        "# Print number of unique entries by column, in ascending order\n",
        "sorted(d.items(), key=lambda x: x[1])"
      ],
      "metadata": {
        "id": "Neyx2ouGJhaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}